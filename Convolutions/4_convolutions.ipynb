{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Assignment 4\n",
    "Previously in 2_fullyconnected.ipynb and 3_regularization.ipynb, we trained fully connected networks to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '../input/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "\n",
    "* convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "\n",
    "* labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape(\n",
    "        (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "    #{Notes: None adds an axis and -1 means unspecified value}\n",
    "    \n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes. So, the model will have the following layers:\n",
    "\n",
    "CONV/RELU/CONV/RELU/FC\n",
    "\n",
    "There are 2 points here worth to mention:\n",
    "\n",
    "### Weight initialization. \n",
    "We want the weights to be very close to zero, but not identically zero. So, it is common to initialize the weights of the neurons with a small amount of noise for symmetry breaking, and to prevent zero gradients. In addition, since we're using ReLU neurons, it is also good practice to initialize them with a slightly positive initial bias to avoid \"dead neurons\".\n",
    "* stddev=0.1\n",
    "\n",
    "### Calculate the size of the output volume in CNN\n",
    "We need to calculate the the size of output volume for weight intialization. From this [link](http://cs231n.github.io/convolutional-networks/), the Conv layer:\n",
    "* Accepts a volume of size W1 x H1 x D1\n",
    "* Requires 4 hyperparameters:\n",
    "    * number of filters K (depth),\n",
    "    * filter size F,\n",
    "    * the stride S,\n",
    "    * amount of zero padding P (\"same\" = -1, \"valid\" = 0)\n",
    "* Produces a volume of size W2 x H2 xD2 where:\n",
    "    * W2 = (W1 - F + 2P)/S + 1\n",
    "    * H2 = (H1 - F + 2P)/S + 1\n",
    "    * D2 = K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "def OS(image_size,patch_size,padding,stride):\n",
    "    ''' A function to estimate the CNN output size \n",
    "        Put padding = -1.0 for 'same' padding and 0.0 for 'valid' padding\n",
    "    '''\n",
    "    os = float(((image_size - patch_size - 2*padding) / stride) + 1.0)\n",
    "    return os\n",
    "\n",
    "# For the architecture above\n",
    "os1 = OS(28, 5, -1.0, 2)\n",
    "os2 = int(np.ceil(OS(os1, 5, -1.0, 2)))\n",
    "print(os2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "        [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "        [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "        [os2 * os2 * depth, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "        [num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "    # Model.\n",
    "    def model(data):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "\n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.843843\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 50: 1.525232\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 59.4%\n",
      "Minibatch loss at step 100: 1.195073\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 68.1%\n",
      "Minibatch loss at step 150: 0.479603\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 200: 0.890921\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 250: 1.151831\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 300: 0.408301\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 350: 0.502805\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 400: 0.269872\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 450: 0.954117\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 500: 0.794962\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 550: 1.029455\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 600: 0.278443\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 650: 0.985443\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 700: 0.908274\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 750: 0.087762\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 800: 0.573290\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 850: 0.979531\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 900: 0.561947\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 950: 0.673791\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 1000: 0.362216\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.9%\n",
      "Test accuracy: 89.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 50 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (nn.max_pool()) of stride 2 and kernel size 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "        [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "        [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "        [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "        [num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "    # Model.\n",
    "    def model(data):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        maxpool = tf.nn.max_pool(hidden,[1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "        conv = tf.nn.conv2d(maxpool, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.986733\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 50: 1.288196\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 58.1%\n",
      "Minibatch loss at step 100: 0.754212\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 71.9%\n",
      "Minibatch loss at step 150: 0.387805\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 200: 0.814225\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 250: 1.174134\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 300: 0.253364\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 350: 0.475776\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 400: 0.238722\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 450: 0.843833\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 500: 0.635246\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 550: 0.708377\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 600: 0.211102\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 650: 0.884504\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 700: 0.967872\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 750: 0.060361\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 800: 0.589521\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 850: 0.977547\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 900: 0.609720\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 950: 0.753550\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 1000: 0.337422\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 83.9%\n",
      "Test accuracy: 90.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 50 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model Structure\n",
    "CONV/RELU/POOL/CONV/RELU/POOL/FC/FC\n",
    "\n",
    "We also apply regularization, learning rate decay, and dropout techniques to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "patch_size = 5; padding_1 = -1.0; padding_2 = 0.0; conv_stride = 1\n",
    "pool_filter_size = 2; pool_stride = 2\n",
    "os1 = OS(image_size, patch_size, padding_2, conv_stride)\n",
    "os2 = OS(os1, pool_filter_size, padding_2, pool_stride)\n",
    "os3 = OS(os2, patch_size, padding_2, conv_stride)\n",
    "os4 = int(np.ceil(OS(os3, pool_filter_size, padding_2, pool_stride)))\n",
    "\n",
    "print (os4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 32\n",
    "num_hidden = 64\n",
    "beta = 0.001\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    '''Input data'''\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    def weight_variable(shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(shape):\n",
    "        initial = tf.zeros(shape)\n",
    "        return tf.Variable(initial) \n",
    "    \n",
    "    weights_layer1 = weight_variable([patch_size, patch_size, num_channels, depth])\n",
    "    biases_layer1 = bias_variable([depth])\n",
    "    \n",
    "    weights_layer2 = weight_variable([patch_size, patch_size, depth, depth])\n",
    "    biases_layer2 = bias_variable([depth])\n",
    "    \n",
    "    weights_layer3 = weight_variable([os4 * os4 * depth, num_hidden])\n",
    "    biases_layer3 = bias_variable([num_hidden])\n",
    "    \n",
    "    weights_layer4 = weight_variable([num_hidden, num_hidden])\n",
    "    biases_layer4 = bias_variable([num_hidden])\n",
    "    \n",
    "    weights_layer5 = weight_variable([num_hidden, num_labels])\n",
    "    biases_layer5 = bias_variable([num_labels])\n",
    "\n",
    "    def cnn_layer(input_tensor, weights, biases, strides, padding):\n",
    "        conv = tf.nn.conv2d(input_tensor, weights, strides, padding)\n",
    "        relu = tf.nn.relu(conv + biases)\n",
    "        return relu\n",
    "\n",
    "    def nn_layer(input_tensor, weights, biases, act=tf.nn.relu):\n",
    "        preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "        activations = act(preactivate)\n",
    "        return activations\n",
    "\n",
    "    def model(data):\n",
    "        conv1 = cnn_layer(data, weights_layer1, biases_layer1, [1, 1, 1, 1], 'VALID')\n",
    "        pool1 = tf.nn.avg_pool(conv1, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID') \n",
    "        conv2 = cnn_layer(pool1, weights_layer2, biases_layer2, [1, 1, 1, 1], 'VALID')  \n",
    "        pool2 = tf.nn.avg_pool(conv2, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID') \n",
    "        shape = pool2.get_shape().as_list()\n",
    "        reshape = tf.reshape(pool2, [shape[0], shape[1] * shape[2] * shape[3]]) \n",
    "        hidden1 = nn_layer(reshape, weights_layer3, biases_layer3)\n",
    "        keep_prob = 0.5\n",
    "        dropped1 = tf.nn.dropout(hidden1, keep_prob)\n",
    "        hidden2 = nn_layer(dropped1, weights_layer4, biases_layer4, act=tf.identity)  \n",
    "        dropped2 = tf.nn.dropout(hidden2, keep_prob)\n",
    "        output = nn_layer(dropped2, weights_layer5, biases_layer5, act=tf.identity)\n",
    "        return output\n",
    "\n",
    "    logits = model(tf_train_dataset) \n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits) +\n",
    "            beta * tf.nn.l2_loss(weights_layer4) + beta * tf.nn.l2_loss(weights_layer5))\n",
    "  \n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.05, global_step, 100000, 0.96, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.394128\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 10.8%\n",
      "Minibatch loss at step 50: 1.967717\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 25.1%\n",
      "Minibatch loss at step 100: 1.741935\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 35.5%\n",
      "Minibatch loss at step 150: 1.274138\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 45.8%\n",
      "Minibatch loss at step 200: 1.277184\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 52.2%\n",
      "Minibatch loss at step 250: 1.255572\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 54.4%\n",
      "Minibatch loss at step 300: 1.398331\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 57.8%\n",
      "Minibatch loss at step 350: 1.005979\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 400: 0.765428\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 450: 0.994168\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 62.6%\n",
      "Minibatch loss at step 500: 1.309602\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 64.1%\n",
      "Minibatch loss at step 550: 0.927200\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 67.2%\n",
      "Minibatch loss at step 600: 0.607568\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 650: 0.719164\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 69.1%\n",
      "Minibatch loss at step 700: 1.532450\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 70.0%\n",
      "Minibatch loss at step 750: 0.450255\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 70.4%\n",
      "Minibatch loss at step 800: 0.793759\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 71.7%\n",
      "Minibatch loss at step 850: 1.048402\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 72.0%\n",
      "Minibatch loss at step 900: 0.612498\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 72.9%\n",
      "Minibatch loss at step 950: 0.764469\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.4%\n",
      "Minibatch loss at step 1000: 0.571980\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 73.8%\n",
      "Minibatch loss at step 1050: 0.890324\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.8%\n",
      "Minibatch loss at step 1100: 0.803602\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 1150: 0.847047\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 74.8%\n",
      "Minibatch loss at step 1200: 1.054866\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 1250: 0.849093\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 1300: 0.445734\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 1350: 1.443379\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 1400: 0.391077\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 1450: 0.618642\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 1500: 1.215937\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 1550: 0.449482\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 1600: 1.511483\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 1650: 0.962217\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 1700: 0.646855\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 1750: 0.605480\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 1800: 0.825867\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 1850: 0.768130\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 1900: 0.577308\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 1950: 0.701120\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch loss at step 2000: 0.316217\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch loss at step 2050: 0.896128\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 2100: 0.372042\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 2150: 0.984825\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 2200: 0.298703\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 2250: 0.829810\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 2300: 0.884096\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 2350: 0.548803\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 2400: 0.810262\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 2450: 0.930640\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 2500: 1.033484\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 2550: 0.827122\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 2600: 0.288260\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 2650: 0.334204\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 2700: 0.978750\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 2750: 1.052788\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 2800: 0.517502\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 2850: 0.131132\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 2900: 0.702085\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 2950: 0.644591\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 3000: 1.051069\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.3%\n",
      "Test accuracy: 88.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 50 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
